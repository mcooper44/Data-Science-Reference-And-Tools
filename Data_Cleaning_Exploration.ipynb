{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code snippets  | notes | references | resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 29)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the auto dataset\n",
    "X = pd.read_csv('auto.csv')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>engine-location</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>length</th>\n",
       "      <th>...</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>city-L/100km</th>\n",
       "      <th>horsepower-binned</th>\n",
       "      <th>diesel</th>\n",
       "      <th>gas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>0.811148</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495</td>\n",
       "      <td>11.190476</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>0.811148</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500</td>\n",
       "      <td>11.190476</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>94.5</td>\n",
       "      <td>0.822681</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500</td>\n",
       "      <td>12.368421</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>0.848630</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>9.791667</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>0.848630</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>13.055556</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized-losses         make aspiration num-of-doors  \\\n",
       "0          3                122  alfa-romero        std          two   \n",
       "1          3                122  alfa-romero        std          two   \n",
       "2          1                122  alfa-romero        std          two   \n",
       "3          2                164         audi        std         four   \n",
       "4          2                164         audi        std         four   \n",
       "\n",
       "    body-style drive-wheels engine-location  wheel-base    length  ...  \\\n",
       "0  convertible          rwd           front        88.6  0.811148  ...   \n",
       "1  convertible          rwd           front        88.6  0.811148  ...   \n",
       "2    hatchback          rwd           front        94.5  0.822681  ...   \n",
       "3        sedan          fwd           front        99.8  0.848630  ...   \n",
       "4        sedan          4wd           front        99.4  0.848630  ...   \n",
       "\n",
       "   compression-ratio  horsepower  peak-rpm city-mpg highway-mpg  price  \\\n",
       "0                9.0       111.0    5000.0       21          27  13495   \n",
       "1                9.0       111.0    5000.0       21          27  16500   \n",
       "2                9.0       154.0    5000.0       19          26  16500   \n",
       "3               10.0       102.0    5500.0       24          30  13950   \n",
       "4                8.0       115.0    5500.0       18          22  17450   \n",
       "\n",
       "  city-L/100km  horsepower-binned  diesel  gas  \n",
       "0    11.190476             Medium       0    1  \n",
       "1    11.190476             Medium       0    1  \n",
       "2    12.368421             Medium       0    1  \n",
       "3     9.791667             Medium       0    1  \n",
       "4    13.055556             Medium       0    1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check for missing values\n",
    "# if only a handful - you could just drop them\n",
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values - if you want...\n",
    "# alternatively you may interpolate or use some other method for filling things in\n",
    "X.dropna(axis=0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Categorical and Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data set - should categorical columns be encoded first or only after splitting.\n",
    "The general consensus is that it should be encoded after splitting, but for a divergent opinion:\n",
    "https://jamesmccaffrey.wordpress.com/2020/05/27/should-you-normalize-and-encode-data-before-train-test-splitting-or-after-splitting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aspiration', 'engine-location']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first it's helpful to see what columns contain what \n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X.columns if X[cname].nunique() < 3 and \n",
    "                        X[cname].dtype == \"object\"]\n",
    "low_cardinality_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables are in columns:\n",
      "['make', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'engine-type', 'num-of-cylinders', 'fuel-system', 'horsepower-binned']\n",
      "Numerical variables are in columns:\n",
      "['symboling', 'normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price', 'city-L/100km', 'diesel', 'gas']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables are in columns:\")\n",
    "print(object_cols)\n",
    "print('Numerical variables are in columns:')\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column header \"make\" contains these unique values...\n",
      "['alfa-romero' 'audi' 'bmw' 'chevrolet' 'dodge' 'honda' 'isuzu' 'jaguar'\n",
      " 'mazda' 'mercedes-benz' 'mercury' 'mitsubishi' 'nissan' 'peugot'\n",
      " 'plymouth' 'porsche' 'renault' 'saab' 'subaru' 'toyota' 'volkswagen'\n",
      " 'volvo']\n",
      "\n",
      "column header \"aspiration\" contains these unique values...\n",
      "['std' 'turbo']\n",
      "\n",
      "column header \"num-of-doors\" contains these unique values...\n",
      "['two' 'four']\n",
      "\n",
      "column header \"body-style\" contains these unique values...\n",
      "['convertible' 'hatchback' 'sedan' 'wagon' 'hardtop']\n",
      "\n",
      "column header \"drive-wheels\" contains these unique values...\n",
      "['rwd' 'fwd' '4wd']\n",
      "\n",
      "column header \"engine-location\" contains these unique values...\n",
      "['front' 'rear']\n",
      "\n",
      "column header \"engine-type\" contains these unique values...\n",
      "['dohc' 'ohcv' 'ohc' 'l' 'rotor' 'ohcf']\n",
      "\n",
      "column header \"num-of-cylinders\" contains these unique values...\n",
      "['four' 'six' 'five' 'three' 'twelve' 'two' 'eight']\n",
      "\n",
      "column header \"fuel-system\" contains these unique values...\n",
      "['mpfi' '2bbl' 'mfi' '1bbl' 'spfi' '4bbl' 'idi' 'spdi']\n",
      "\n",
      "column header \"horsepower-binned\" contains these unique values...\n",
      "['Medium' 'Low' 'High' nan]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print unique values in the categorical columns\n",
    "# NB - if there is a test set - compare the values in the test and training sets to\n",
    "# ensure that the intersection of both sets is complete otherwise the encoding steps\n",
    "# will throw an error\n",
    "for o in object_cols:\n",
    "    print(f'column header \"{o}\" contains these unique values...')\n",
    "    print(X[o].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the categorical columns values is there an inherent ranking present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap values with a dictionary manually with replace\n",
    "X.replace({\"num-of-doors\":{'two':2, 'four':4},\n",
    "           \"num-of-cylinders\":{'four':4, 'six':6, 'five':5, 'three':3, 'twelve':12, 'two':2, 'eight':8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "\n",
    "and since it is a blind spot in the documentation - for a clear explanation on how to use categories\n",
    "https://datascience.stackexchange.com/questions/72343/encoding-with-ordinalencoder-how-to-give-levels-as-user-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# for one column using categories\n",
    "#ordinal_encoder = OrdinalEncoder(categories=[['two','four']])\n",
    "#X_copy = ordinal_encoder.fit_transform(X.loc[:,[\"num-of-doors\"]])\n",
    "\n",
    "# for using multiple - put the labels in order in the lists\n",
    "door_cats = ['two', 'four']\n",
    "cylinder_cats = ['two','three','four', 'five', 'six','eight','twelve']\n",
    "horse_cats = ['Low', 'Medium', 'High']\n",
    "\n",
    "# and then feed them to the encoder class and use the fit_transform method\n",
    "ordinal_encoder = OrdinalEncoder(categories=[door_cats,cylinder_cats,horse_cats])\n",
    "X[['num-of-doors', 'num-of-cylinders']] = ordinal_encoder.fit_transform(X[['num-of-doors', \n",
    "                                                                           'num-of-cylinders',\n",
    "                                                                           'horsepower-binned']])\n",
    "\n",
    "# or to let the ordinal encoder lable things automatically...\n",
    "#label_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>num-of-cylinders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num-of-doors  num-of-cylinders\n",
       "0             0.0               2.0\n",
       "1             0.0               2.0\n",
       "2             0.0               4.0\n",
       "3             1.0               2.0\n",
       "4             1.0               3.0\n",
       "..            ...               ...\n",
       "196           1.0               2.0\n",
       "197           1.0               2.0\n",
       "198           1.0               4.0\n",
       "199           1.0               4.0\n",
       "200           1.0               2.0\n",
       "\n",
       "[201 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['num-of-doors', 'num-of-cylinders','horsepower-binned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the original values\n",
    "ordinal_encoder.inverse_transform(X[['num-of-doors', 'num-of-cylinders']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.price\n",
    "X.drop(['price'], axis=1, inplace=True) \n",
    "# if assigning to another variable remove the inplace\n",
    "# X = X_train.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check to make sure nothing is missing\n",
    "y_miss = y.isna().sum().sum()\n",
    "x_miss = X.isna().sum().sum()\n",
    "print(f'X missing values: {y_miss} Y missing values: {x_miss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in numerical_cols:\n",
    "    print('column header', n, 'is numerical and has these stats:')\n",
    "    print('mean', X_train[n].mean())\n",
    "    print('median', X_train[n].median())\n",
    "    print('std deviation', X_train[n].std())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all categorical variables from the dataset\n",
    "#drop_X_train = X_train.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns in the training data\n",
    "# have missing values?\n",
    "missing_count = (X_train.isnull().sum())\n",
    "num_cols_with_missing = missing_count[missing_count > 0].count()\n",
    "\n",
    "# Fill in the line below: How many missing entries are contained in \n",
    "# all of the training data?\n",
    "tot_missing = X_train.isna().sum().sum()\n",
    "print(f'num_rows: {num_rows} num_columns: {num_columns}')\n",
    "print(f'number of columns with missing values: {num_cols_with_missing}')\n",
    "print(f'total number of missing values: {tot_missing}')\n",
    "print()\n",
    "print('columns with missing values + count of missing')\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "cols_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/docs/user_guide/missing_data.html#missing-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns in training and validation data with axis=1 \n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_train\n",
    "\n",
    "# alternate\n",
    "# reduced_X_train = X_train.dropna(how='any') #'all' = only drop if all of a row or column is na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows in training and validation data with missing values\n",
    "# 0, or â€˜indexâ€™ : Drop rows which contain missing values.\n",
    "# 1, or â€˜columnsâ€™ : Drop columns which contain missing value.\n",
    "X_train.dropna(axis=0, inplace=True)\n",
    "X_valid.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = X_train.shape[0]\n",
    "num_columns = X_train.shape[1]\n",
    "print('df is: ',num_rows, 'by', num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove categorical columns with values that don't map onto testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that could be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "\n",
    "OH_cols_train # this is col 'four' with the five different values translated\n",
    "              # into a column for the five differnt values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an imputer to fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "### BASIC EXAMPLE ###\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train.select_dtypes(exclude=['object'])))\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.select_dtypes(exclude=['object']).columns\n",
    "imputed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD A COLUMN TO IDENTIFY WHICH VALUES WERE MISSING ###\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.select_dtypes(exclude=['object']).copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in imputed_X_train:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    \n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_X_train_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====|PIPELINES|===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(r'C:\\Users\\John Rando\\Documents\\Code\\Kaggle\\melbourne-housing-snapshot\\melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the cross-validation scores with the cross_val_score() function from scikit-learn. We set the number of folds with the cv parameter.\n",
    "\n",
    "The scoring parameter chooses a measure of model quality to report: in this case, we chose negative mean absolute error (MAE). The docs for scikit-learn show a list of options.\n",
    "\n",
    "It is a little surprising that we specify negative MAE. Scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere.\n",
    "\n",
    "We typically want a single measure of model quality to compare alternative models. So we take the average across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    # Replace this body with your own code\n",
    "    a_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators=n_estimators, random_state=0))])\n",
    "    a_score = -1 * cross_val_score(a_pipeline, X, y,\n",
    "                              cv=3,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "    return a_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score models with different numbers of esitmators, then plot the scores and look for the elbow\n",
    "results = {}\n",
    "for n in range(50, 450, 50):\n",
    "    results[n] = get_score(n)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
